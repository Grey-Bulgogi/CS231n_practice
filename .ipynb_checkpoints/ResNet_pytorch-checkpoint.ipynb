{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 주요내용\n",
    "\n",
    "- Deep한 모델을 학습시킬 때 나타나는 Degradation 문제를 해결하기 위해 제시된 방법\n",
    "    - *Degradation*: 망이 깊어지면서 vanishing, exploding gradient가 발생하여 학습이 어려워지는 문제\n",
    "    - Residual learning의 효과 입증\n",
    "    - Vanila 형태와 Bottleneck형태가 있음\n",
    "    \n",
    "- Depp한 모델보다 더욱 Deep한 모델을 훈련시키려고 함.\n",
    "    - Vanishing, exploding 문제가 발생 -> 해결하기 위해 초기 wegith 정구화 또는 중간 noramlizaiton등 연구됨\n",
    "    - Deep 모델이 수렴하기 시작할 때, 발생하는 degradation문제는 overfitting 때문에 그런것이 아니라고 주장.\n",
    "\n",
    "\n",
    "- 주요 컨셉\n",
    "    - res 학습의 주요 컨셉은 F(X):= H(X) - x 처럼 output과 input의 차이인 F(x)를 학습하는 것이다.\n",
    "        - 만약 H(x)가 identity function이라면 F(x)를 0으로 학습시키는 것이 더용이하다. // F(x):= 0 (if, H(x)= x)\n",
    "        \n",
    "    - H(x) = F(x) + x 는 또한 shortcut connection으로도 이해가 가능하다. \n",
    "        - 1개 또는 그 이상의 layer들을 파라미터 없이 바로 skip하여 연결하는 개념.\n",
    "        - `즉, 훈련시 몇개의 layer들을 x라는 identity matrix를 통해 스킵할 수 있다는 개념이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논문에서 사용된 ResNet 34-layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, in_size, out_size, dim_inc=False, stride=1):\n",
    "        super(Residual_block, self).__init__()\n",
    "                \n",
    "        # plain block\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_size, out_size, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        \n",
    "        #padding block\n",
    "        self.pad = nn.Sequential(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=1, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if dim_inc == True:\n",
    "            shortcut = self.pad(x)\n",
    "        else:\n",
    "            shortcut = x\n",
    "            \n",
    "        out = self.block(x)\n",
    "        out += x\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "                               \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, image_shape, n_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # 64 * row * height\n",
    "        self.prev_layer = nn.Sequential(\n",
    "            nn.Conv2d(image_shape[0], 64, kernel_size=7, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.conv2n = nn.Sequential(\n",
    "            Residual_block(64, 64, False),\n",
    "            Residual_block(64, 64, False),\n",
    "            Residual_block(64, 64, False)\n",
    "        )\n",
    "        \n",
    "        self.conv3n = nn.Sequential(\n",
    "            Residual_block(64, 128, True),\n",
    "            Residual_block(128, 128, False),\n",
    "            Residual_block(128, 128, False),\n",
    "            Residual_block(128, 128, False)\n",
    "        )\n",
    "        \n",
    "        self.conv4n = nn.Sequential(\n",
    "            Residual_block(128, 256, True),\n",
    "            Residual_block(256, 256, False),\n",
    "            Residual_block(256, 256, False),\n",
    "            Residual_block(256, 256, False),\n",
    "            Residual_block(256, 256, False),\n",
    "            Residual_block(256, 256, False)\n",
    "        )\n",
    "        \n",
    "        self.conv5n = nn.Sequential(\n",
    "            Residual_block(256, 512, True),\n",
    "            Residual_block(512, 512, False),\n",
    "            Residual_block(512, 512, False),\n",
    "            Residual_block(512, 512, False),\n",
    "            Residual_block(512, 512, False),\n",
    "            Residual_block(512, 512, False)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.clssifier = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=1),\n",
    "            nn.Linear(512, 1000)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.prev_layer(x)\n",
    "        out = self.conv2n(out)\n",
    "        out = self.conv3n(out)\n",
    "        out = self.conv4n(out)\n",
    "        out = self.conv5n(out)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "           \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (prev_layer): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2n): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv3n): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv4n): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (conv5n): Sequential(\n",
       "    (0): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): Residual_block(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (pad): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "        (1): ReLU()\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (clssifier): Sequential(\n",
       "    (0): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "    (1): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResNet((3, 256, 256))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "\n",
    "- https://dnddnjs.github.io/cifar10/2018/10/09/resnet/\n",
    "- https://github.com/GunhoChoi/PyTorch-FastCampus/blob/master/04_CNN_Advanced/3_ResNet.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
