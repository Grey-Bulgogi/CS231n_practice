{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier\n",
    "    - with SVM loss\n",
    "    - with Softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### required methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(data):\n",
    "    name_dict = {name: val  for val, name in enumerate(np.unique(data))}\n",
    "    encoded_data = list(map(lambda x: name_dict[x], data))\n",
    "    return np.array(encoded_data)\n",
    "\n",
    "def data_split(data, label, frac=0.7):\n",
    "    \n",
    "    num_of_label = len(np.unique(label))\n",
    "    count = [ np.sum(y_data == i) for i in range(num_of_label)]  # 각 label의 갯수\n",
    "    prop = [int(each * frac) for each in count]  # 각 label의 비율\n",
    "    \n",
    "    train_idx = [np.where(label == each)[0][:prop[each]] for each in range(num_of_label)]  # 비율에 맞게 모은 인덱스\n",
    "    test_idx = [np.where(label == each)[0][prop[each]:] for each in range(num_of_label)]  # 비율에 맞게 모은 인덱스\n",
    "\n",
    "    X_train = np.concatenate([data[each] for each in train_idx])\n",
    "    y_train = np.concatenate([label[each] for each in train_idx])\n",
    "    \n",
    "    X_test = np.concatenate([data[each] for each in test_idx])\n",
    "    y_test = np.concatenate([label[each] for each in test_idx])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load: Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((29394, 784), (29394,), (12606, 784), (12606,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:/Users/anjae/Documents/00_Dataset/mnist/train.csv'\n",
    "DF = pd.read_csv(path)\n",
    "\\\n",
    "X_data, y_data = DF.drop(['label'], axis=1).values, DF.label.values\n",
    "X_train, y_train, X_test, y_test = data_split(X_data, y_data)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load: iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((105, 4), (105,), (45, 4), (45,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'C:/Users/anjae/Documents/00_Dataset/iris/iris.csv'\n",
    "DF = pd.read_csv(path)\n",
    "\n",
    "# X_data, y_data = DF.drop(['Species','caseno', 'PetalWidth'], axis=1).values, label_encode(DF.Species.values)\n",
    "X_data, y_data = DF.drop(['Species','caseno'], axis=1).values, label_encode(DF.Species.values)\n",
    "X_train, y_train, X_test, y_test = data_split(X_data, y_data)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(object):\n",
    "    def __init__(self, shape, num_classes, model='svm', delta = 1.0, gamma = 1, lr=0.01, epoch = 1000):\n",
    "        \n",
    "        self.w = np.random.uniform(0, 1, [num_classes, shape]) # weight matrix 생성\n",
    "        self.b = np.zeros(num_classes)\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epoch = epoch\n",
    "        self.model = model\n",
    "        self.epoch = epoch\n",
    "        \n",
    "        if model == 'svm':\n",
    "            self.delta = delta\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        for _ in range(self.epoch):\n",
    "            dw = np.zeros_like(self.w)\n",
    "            db = np.zeros_like(range(len(np.unique(y_train))))  # 나중에 고쳐\n",
    "            \n",
    "            if self.model == 'softmax':\n",
    "                # forward\n",
    "                scores = X_train.dot(self.w.T) + self.b\n",
    "                exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "                probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "                correct_logprobs = -np.log(probs[np.arange(len(X_train)), y_train])\n",
    "                              \n",
    "                data_loss = np.sum(correct_logprobs) / len(X_train)\n",
    "                reg_loss = 0.5 * self.gamma * np.sum(self.w*self.w)  # 1/2 * gamma * l2Norm(w)\n",
    "                loss = data_loss + reg_loss\n",
    "                                \n",
    "                # backward\n",
    "                dscores = probs.copy()    \n",
    "                dscores[np.arange(len(X_train)), y_train] -= 1  # 전체 데이터셋에 prob -1 취하기\n",
    "                \n",
    "                #print(dscores)\n",
    "                \n",
    "                \n",
    "                dscores /= len(X_train)  # 미리 나누어준다\n",
    "                \n",
    "                #print(dscores)\n",
    "                \n",
    "                dw = dscores.T.dot(X_train)\n",
    "                #dw = X_train.dot(dscores.T)   # x * (q_i - 1) for all dataset\n",
    "                dw += self.gamma * self.w     # reg gradient\n",
    "                db = np.sum(dscores, axis = 0, keepdims=True)\n",
    "                               \n",
    "                self.w = self.w - (self.lr*dw)\n",
    "                self.b = self.b - (self.lr*db)\n",
    "                \n",
    "            else:\n",
    "                # SVM\n",
    "                # forward\n",
    "                scores = X_train.dot(self.w.T)\n",
    "                correct_scores = np.array(scores[np.arange(len(X_train)), y_train]).reshape(len(X_train),-1)\n",
    "            \n",
    "                margins = scores - correct_scores + self.delta\n",
    "                margins[np.arange(len(X_train)), y_train] = 0\n",
    "                \n",
    "                \n",
    "                data_loss = np.sum(np.maximum(margins, 0)) / len(X_train)\n",
    "                reg_loss = 0.5 * self.gamma * np.sum(self.w * self.w)\n",
    "                \n",
    "                loss = data_loss + reg_loss\n",
    "                \n",
    "                # backward\n",
    "                for i, each in enumerate(margins):\n",
    "                    \n",
    "                    idx = np.where(each > 0)[0]\n",
    "                    \n",
    "                    # margin이 있으면 실행, 없으면 실행 안함\n",
    "                    for j in idx:\n",
    "                        dw[y_train[i],:] += -X_train[i, :]\n",
    "                        dw[j, :] += X_train[i, :]\n",
    "                        \n",
    "                dw += self.gamma * self.w\n",
    "                self.w = self.w - (self.lr * dw)\n",
    "                \n",
    "    def predict(self, X_test, y_test):\n",
    "        if self.model == 'softmax':\n",
    "            scores = X_test.dot(self.w.T) + self.b\n",
    "            normalized_scores = scores - np.max(scores, axis=1, keepdims=True)\n",
    "            exp_scores = np.exp(normalized_scores)\n",
    "            probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "            predict = np.argmax(probs, axis = 1)\n",
    "        else:\n",
    "            total = len(y_test)\n",
    "            count = 0\n",
    "            scores = X_test.dot(self.w.T)\n",
    "            predict = np.argmax(scores, axis=1)\n",
    "\n",
    "        print(np.sum(predict == y_test) / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_train))\n",
    "n_smaples = X_train.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "clf = LinearModel(n_features, n_classes, model='svm', epoch=1000, delta= 10, gamma=0.5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc\n",
      "0.7555555555555555\n"
     ]
    }
   ],
   "source": [
    "print('Acc')\n",
    "clf.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_train))\n",
    "n_smaples = X_train.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "clf = LinearModel(n_features, n_classes, model='softmax', epoch=2000)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc\n",
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "print('Acc')\n",
    "clf.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference - SVM / Hinge loss / GD\n",
    "- https://stats.stackexchange.com/questions/155088/gradient-for-hinge-loss-multiclass\n",
    "- https://cs231n.github.io/linear-classify/#svm\n",
    "- http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf\n",
    "- https://zhuanlan.zhihu.com/p/30965514\n",
    "- https://bruceoutdoors.wordpress.com/2016/05/06/cs231n-assignment-1-tutorial-q2-training-a-support-vector-machine/\n",
    "- http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference - softmax / cross_entropy\n",
    "- http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n",
    "- https://madalinabuzau.github.io/2016/11/29/gradient-descent-on-a-softmax-cross-entropy-cost-function.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2.0",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
