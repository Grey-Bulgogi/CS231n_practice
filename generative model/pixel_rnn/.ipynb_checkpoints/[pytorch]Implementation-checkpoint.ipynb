{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size_train = 16\n",
    "batch_size_test = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=True, download=True,\n",
    "                              transform = torchvision.transforms.Compose(\n",
    "                                  [torchvision.transforms.ToTensor(),\n",
    "                                  ])),\n",
    "    batch_size = batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./files/', train=False, download=True,\n",
    "                              transform = torchvision.transforms.Compose(\n",
    "                                  [torchvision.transforms.ToTensor(),\n",
    "                                  ])),\n",
    "    batch_size = batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mutable vs immutable \n",
    "- mutable복사는 리스트를 갖고있는 변수 a를 b로 복사하여도 a와 b가 같은 주소를 갖고있는 것.\n",
    "- immutable은 a를 복사하여 같은 데이터를 b도 같지만 주소가 다른 것.\n",
    "\n",
    "\n",
    "##### shallow copy vs depp copy\n",
    "- shallow copy는 mutable한 경우를 말한다.\n",
    "- deep copy는 immutable한 경우를 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(module, N):\n",
    "    \"\"\"\n",
    "    같은 모듈을 계속 복사해주는 메소드.\n",
    "    그러나, 모두 다른 주소값을 갖는 새로 생성된 모듈이다.\n",
    "    \n",
    "    INPUT: module\n",
    "    OUTPUT: A list of the same modules \n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv2d(nn.Conv2d):\n",
    "    def __init__(self, *args, mask='B', **kargs):\n",
    "        super(MaskedConv2d, self).__init__(*args, **kargs)\n",
    "        assert mask in {'A', 'B'}\n",
    "        self.mask_type = mask\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        self.mask.fill_(1)\n",
    "        \n",
    "        \n",
    "        _, _, H, W = self.mask.size()\n",
    "    \n",
    "        self.mask[:, :, H//2, W//2 + (self.mask_type == 'B'):] = 0  \n",
    "        self.mask[:, :, H//2+1:, :] = 0\n",
    "        \n",
    "        \"\"\"\n",
    "                                           11, 12번 라인 적용\n",
    "        tensor([[[[1., 1., 1., 1., 1.],                    tensor([[[[1., 1., 1., 1., 1.],\n",
    "                  [1., 1., 1., 1., 1.],                              [1., 1., 1., 1., 1.],\n",
    "                  [1., 1., 1., 1., 1.],                              [1., 1., 1., 0., 0.],\n",
    "                  [1., 1., 1., 1., 1.],                              [0., 0., 0., 0., 0.],\n",
    "                  [1., 1., 1., 1., 1.]]]])                           [0., 0., 0., 0., 0.]]]]) \n",
    "        \n",
    "          \"\"\"\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask      # self.weight에 mask 씌어줄려고 mask생성 한거구나\n",
    "                                           # 그 해당 부분에만 가중치가 생기게 할려고\n",
    "        #print(self.weight.data)\n",
    "        \n",
    "        return super(MaskedConv2d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, mask='B', **kargs):\n",
    "        super(MaskedConv1d, self).__init__(*args, **kargs)\n",
    "        assert mask in {'A', 'B'}\n",
    "        self.mask_type = mask\n",
    "        self.register_buffer('mask', self.weight.data.clone())\n",
    "        self.mask.fill_(1)\n",
    "    \n",
    "        _, _, W = self.mask.size()\n",
    "    \n",
    "        self.mask[:, :, W//2 + (self.mask_type == 'B'):] = 0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super(MaskedConv1d, self).forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8702]]]], grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = torch.Tensor(np.ones((1,1,5,5)))\n",
    "a, b, c, d = sample.size()\n",
    "\n",
    "model = MaskedConv2d(a, b, c, d)\n",
    "model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _padding(i, o, k, s=1, d=1, mode='same'):\n",
    "    if mode == 'same':\n",
    "        return ((o-1) * s + (k-1)*(d-1) + k - i) // 2\n",
    "    else:\n",
    "        raise RuntimeError('Not implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.8049, 1.0001, 1.0001, 1.0001, 1.0001],\n",
       "          [0.8049, 1.0001, 1.0001, 1.0001, 1.0001],\n",
       "          [0.8049, 1.0001, 1.0001, 1.0001, 1.0001],\n",
       "          [0.8049, 1.0001, 1.0001, 1.0001, 1.0001],\n",
       "          [0.8049, 1.0001, 1.0001, 1.0001, 1.0001]]]],\n",
       "       grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MaskedConv2d(1,1,(1,3), padding=(0,_padding(5,5,3)))(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowLSTMCell(nn.Module):\n",
    "    def __init__(self, hidden_dims, image_size, channel_in, *args, **kargs):\n",
    "        super(RowLSTMCell, self).__init__(*args, **kargs)\n",
    "\n",
    "        self._hidden_dims = hidden_dims\n",
    "        self._image_size = image_size\n",
    "        self._channel_in = channel_in\n",
    "        self._num_units = self._hidden_dims * self._image_size\n",
    "        self._output_size = self._num_units\n",
    "        self._state_size = self._num_units * 2\n",
    "\n",
    "        self.conv_i_s = MaskedConv1d(self._hidden_dims, 4 * self._hidden_dims, 3, mask='B', padding=_padding(image_size, image_size, 3))\n",
    "        self.conv_s_s = nn.Conv1d(channel_in, 4 * self._hidden_dims, 3, padding=_padding(image_size, image_size, 3))\n",
    "   \n",
    "    def forward(self, inputs, states):\n",
    "        c_prev, h_prev = states\n",
    "\n",
    "\n",
    "\n",
    "        h_prev = h_prev.view(-1, self._hidden_dims,  self._image_size)\n",
    "        inputs = inputs.view(-1, self._channel_in, self._image_size)\n",
    "\n",
    "        s_s = self.conv_s_s(h_prev) #(batch, 4*hidden_dims, width)\n",
    "        i_s = self.conv_i_s(inputs) #(batch, 4*hidden_dims, width)\n",
    "\n",
    "\n",
    "\n",
    "        s_s = s_s.view(-1, 4 * self._num_units) #(batch, 4*hidden_dims*width)\n",
    "        i_s = i_s.view(-1, 4 * self._num_units) #(batch, 4*hidden_dims*width)\n",
    "\n",
    "        #print(s_s.size(), i_s.size())\n",
    "\n",
    "        lstm = s_s + i_s\n",
    "\n",
    "        lstm = torch.sigmoid(lstm)\n",
    "\n",
    "        i, g, f, o = torch.split(lstm, (4 * self._num_units)//4, dim=1)\n",
    "\n",
    "        c = f * c_prev + i * g\n",
    "        h = o * torch.tanh(c)\n",
    "\n",
    "        new_state = (c, h)\n",
    "        return h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dims, input_size, channel_in, *args, init='zero', **kargs):\n",
    "        super(RowLSTM, self).__init__(*args, **kargs)\n",
    "        assert init in {'zero', 'noise', 'variable', 'variable noise'}\n",
    "\n",
    "        self.init = init\n",
    "        self._hidden_dims = hidden_dims\n",
    "        #self.return_state = return_state\n",
    "        if self.init == 'zero':\n",
    "            self.init_state = (torch.zeros(1, input_size * hidden_dims), torch.zeros(1, input_size * hidden_dims))\n",
    "        elif self.init == 'noise':\n",
    "            self.init_state = (torch.Tensor(1, input_size * hidden_dims), torch.Tensor(1, input_size * hidden_dims))\n",
    "            nn.init.uniform(self.init_state[0])\n",
    "            nn.init.uniform(self.init_state[1])  \n",
    "        elif self.init == 'variable':\n",
    "            hidden0 = torch.zeros(1,input_size * hidden_dims)\n",
    "            ##if use_cuda:\n",
    "            ##  hidden0 = hidden0.cuda()\n",
    "            ##else:\n",
    "            ##  hidden0 = hidden0\n",
    "            self._hidden_init_state = torch.nn.Parameter(hidden0, requires_grad=True)\n",
    "            self._cell_init_state = torch.nn.Parameter(hidden0, requires_grad=True)\n",
    "            self.init_state = (self._hidden_init_state, self._cell_init_state)\n",
    "        else:\n",
    "            hidden0 = torch.Tensor(1, input_size * hidden_dims) # size\n",
    "            nn.init.uniform(hidden0)\n",
    "            self._hidden_init_state = torch.nn.Parameter(hidden0, requires_grad=True)\n",
    "            self._cell_init_state = torch.nn.Parameter(hidden0, requires_grad=True)\n",
    "            self.init_state = (self._hidden_init_state, self._cell_init_state)\n",
    "\n",
    "        self.lstm_cell = RowLSTMCell(hidden_dims, input_size, channel_in)\n",
    "    \n",
    "    def forward(self, inputs, initial_state=None):\n",
    "        '''\n",
    "        states --> (c, h), tuple\n",
    "        c,h --> (batch, width * hidden_dims)\n",
    "        inputs --> (batch, seq_length, input_shape)\n",
    "        input_shape --> width, channel\n",
    "        '''\n",
    "\n",
    "\n",
    "        n_batch, channel, n_seq, width = inputs.size()\n",
    "        #print(n_seq)\n",
    "        #inputs = inputs.view(n_batch, channel, n_seq, width)\n",
    "        if initial_state is None:\n",
    "            hidden_init, cell_init = self.init_state\n",
    "\n",
    "        else:\n",
    "            hidden_init, cell_init = initial_state\n",
    "\n",
    "        states = (hidden_init.repeat(n_batch,1), cell_init.repeat(n_batch, 1))\n",
    "\n",
    "        steps = [] # --> (batch, width * hidden_dims) --> (batch, 1, width*hidden_dims)\n",
    "        for seq in range(n_seq):\n",
    "            #print(inputs[:, :, seq, :].size())\n",
    "            h, states = self.lstm_cell(inputs[:, :, seq, :], states)\n",
    "            steps.append(h.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(steps, dim=1).view(-1, n_seq, width, self._hidden_dims).permute(0,3,1,2) # --> (batch, seq_length(a.k.a height), width * hidden_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lstm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-04744ff5aafb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lstm' is not defined"
     ]
    }
   ],
   "source": [
    "for p in lstm.parameters():\n",
    "    print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 28 28 1\n"
     ]
    }
   ],
   "source": [
    "batch_sample=torch.Tensor(np.random.random((16,28,28, 1)))\n",
    "b, c, h, w = batch_sample.size()\n",
    "print(b,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = RowLSTM(32, 28, 1, init='variable')\n",
    "#lstm(batch_sample, (torch.Tensor(np.random.random((16, 28 * 32))),torch.Tensor(np.random.random((16, 28 * 32))))).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelRNN(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_dims, input_size, *args, **kargs):\n",
    "        super(PixelRNN, self).__init__(*args, **kargs)\n",
    "\n",
    "        pad_conv1 = _padding(input_size, input_size, 7)\n",
    "        pad_conv2 = _padding(input_size, input_size, 1)\n",
    "        self.conv1 = MaskedConv2d(1, hidden_dims, (7,7), mask='A', padding=(pad_conv1, pad_conv1))\n",
    "        self.lstm_list = nn.ModuleList([RowLSTM(hidden_dims, input_size, hidden_dims) for _ in range(num_layers)])\n",
    "        self.conv2 = nn.Conv2d(hidden_dims, 32, (1,1), padding=(pad_conv2, pad_conv2))\n",
    "        self.conv_last = nn.Conv2d(32, 1, (1,1), padding=(pad_conv2, pad_conv2))\n",
    "    \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        for lstm in self.lstm_list:\n",
    "            x = lstm(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv_last(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PixelRNN(num_layers=2, hidden_dims=16, input_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PixelRNN(\n",
       "  (conv1): MaskedConv2d(1, 16, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "  (lstm_list): ModuleList(\n",
       "    (0): RowLSTM(\n",
       "      (lstm_cell): RowLSTMCell(\n",
       "        (conv_i_s): MaskedConv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (conv_s_s): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): RowLSTM(\n",
       "      (lstm_cell): RowLSTMCell(\n",
       "        (conv_i_s): MaskedConv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (conv_s_s): Conv1d(16, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv_last): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = optim.RMSprop(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산에 필요한 파라미터 구하기\n",
    "- 파라미터 이름과 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([16, 1, 7, 7])\n",
      "conv1.bias torch.Size([16])\n",
      "lstm_list.0.lstm_cell.conv_i_s.weight torch.Size([64, 16, 3])\n",
      "lstm_list.0.lstm_cell.conv_i_s.bias torch.Size([64])\n",
      "lstm_list.0.lstm_cell.conv_s_s.weight torch.Size([64, 16, 3])\n",
      "lstm_list.0.lstm_cell.conv_s_s.bias torch.Size([64])\n",
      "lstm_list.1.lstm_cell.conv_i_s.weight torch.Size([64, 16, 3])\n",
      "lstm_list.1.lstm_cell.conv_i_s.bias torch.Size([64])\n",
      "lstm_list.1.lstm_cell.conv_s_s.weight torch.Size([64, 16, 3])\n",
      "lstm_list.1.lstm_cell.conv_s_s.bias torch.Size([64])\n",
      "conv2.weight torch.Size([32, 16, 1, 1])\n",
      "conv2.bias torch.Size([32])\n",
      "conv_last.weight torch.Size([1, 32, 1, 1])\n",
      "conv_last.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "accum = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 축적된 파라미터 총 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13921\n"
     ]
    }
   ],
   "source": [
    "accum = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        sz = param.size()\n",
    "        tmp = 1\n",
    "        for i in sz:\n",
    "            tmp *= i\n",
    "        accum += tmp\n",
    "    \n",
    "print(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x196da0f52c8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binarize(image):\n",
    "    # 평균 0, 편차 1의 정규분포에서 샘플링한 image.shape에서 image보다 픽셀값이 작으면 True, 크며 False\n",
    "    return (np.random.uniform(0, 1, image.shape) < image).astype('float32')\n",
    "    \n",
    "    \n",
    "binarize(np.array([0.5,0.2,0.4, 0.6, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anjae\\Anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\anjae\\Anaconda3\\envs\\torch\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Training Loss:  0.10062909040848414 Valid Loss:  nan\n",
      "Epoch: 2 Training Loss:  0.08875046478907267 Valid Loss:  nan\n",
      "Epoch: 3 Training Loss:  0.08783200233777363 Valid Loss:  nan\n",
      "Epoch: 4 Training Loss:  0.08740872054298719 Valid Loss:  nan\n",
      "Epoch: 5 Training Loss:  0.08718138246734937 Valid Loss:  nan\n",
      "Epoch: 6 Training Loss:  0.08697619158824285 Valid Loss:  nan\n",
      "Epoch: 7 Training Loss:  0.08680357954303423 Valid Loss:  nan\n",
      "Epoch: 8 Training Loss:  0.08674448099335035 Valid Loss:  nan\n",
      "Epoch: 9 Training Loss:  0.08658883761763572 Valid Loss:  nan\n",
      "Epoch: 10 Training Loss:  0.0865074438949426 Valid Loss:  nan\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11): ## run the model for 10 epochs\n",
    "    train_loss, valid_loss = [], []\n",
    "    ## training part \n",
    "    model.train()\n",
    "    for data, _ in train_loader:\n",
    "        \n",
    "        x = binarize(data.numpy())\n",
    "        x = torch.Tensor(x)\n",
    "        y = data.clone()\n",
    "      \n",
    "      \n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        ## 1. forward propagation\n",
    "        output = model(x)\n",
    "        \n",
    "        ## 2. loss calculation\n",
    "        loss = loss_function(output, y)\n",
    "        \n",
    "        ## 3. backward propagation\n",
    "        loss.backward()\n",
    "        \n",
    "        ## 4. weight optimization\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss), \"Valid Loss: \", np.mean(valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reference\n",
    "\n",
    "- 파이토치 구현 참고 https://github.com/heechan95/PixelRNN-pytorch/blob/master/PixelRNN%20pytorch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
